<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="index, follow">

    <title>Gao's LAB</title>
    <meta name="description" content="">
    <link rel="alternate" type="application/rss+xml" href="/feed.xml">

    <!-- Stylesheet -->
    <link rel="stylesheet" href="/css/main.css" rel='stylesheet' type='text/css'>


    <!-- Google Fonts -->
    <link href='https://fonts.googleapis.com/css?family=Nunito:400,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic' rel='stylesheet' type='text/css'>

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/small_logo.png">
</head>


<body data-spy="scroll" data-offset="80" data-target=".scrollspy" id="top">
    <div class="navigation"></div>

    <div class="news top-container">
        <div class="container-fluid">
            <div id="primarycontent">
                <center>
                    <h2>RGBD Based Gaze Estimation via Multi-task CNN</h2>
                </center>
                <center>
                    <h3>
                        <a href="">Dongze Lian</a>&nbsp;&nbsp;&nbsp;
                        <a href="">Ziheng Zhang</a>&nbsp;&nbsp;&nbsp;
                        <a href="">Weixin Luo</a>
                        <a href="">Lina Hu</a>
                        <a href="">Minye Wu</a>
                        <a href="">Zechao Li</a>
                        <a href="">Jingyi Yu</a>
                        <a href="">Shenghua Gao</a>
                    </h3>
                </center>
                <center>
                    <h3>
                        <a href="http://www.shanghaitech.edu.cn" target="_blank">ShanghaiTech University</a>
                    </h3>
                </center>
                <center>
                    <h3 style="color: #000000">In AAAI 2019</h2>
                </center>
                <!-- <center>
                    <h3>
                        <a href="https://arxiv.org/abs/1712.09867" target="_blank" style="color: #990000">[Paper]</a>
                        <a href="https://github.com/StevenLiuWen/ano_pred_cvpr2018" target="_blank" style="color: #990000">[Code (Tensorflow)]</a>
                    </h3>
                </center> -->
                <center>
                    <img src="../img/project/aaai2019_liandz.png" width="1000">
                </center>
                <p></p>


                <p>
                    <h2>Abstract</h2>

                    <div style="font-size:14px">
                        This paper tackles RGBD based gaze estimation with Convolutional Neural Networks (CNNs).
                        Specifically, we propose to decompose gaze point estimation into eyeball pose, head pose, and
                        3D eye position estimation. Compared with RGB image-based gaze tracking, having depth modality
                        helps to facilitate head pose estimation and 3D eye position estimation. The captured depth
                        image, however, usually contains noise and black holes which noticeably hamper gaze tracking.
                        Thus we propose a CNN-based multi-task learning framework to simultaneously refine depth images
                        and predict gaze points. We utilize a generator network for depth image generation with a
                        Generative Neural Network (GAN), where the generator network is partially shared by both the
                        gaze tracking network and GAN-based depth synthesizing. By optimizing the whole network
                        simultaneously, depth image synthesis improves gaze point estimation and vice versa. Since the
                        only existing RGBD dataset (EYEDIAP) is too small, we build a large-scale RGBD gaze tracking
                        dataset for performance evaluation. As far as we know, it is the largest RGBD gaze dataset in
                        terms of the number of participants. Comprehensive experiments demonstrate that our method
                        outperforms existing methods by a large margin on both our dataset and the EYEDIAP dataset.
                </p>
            </div>

            <!-- <h2>Citation</h2>
                    If you find this useful, please cite our work as follows:

                    <pre>
@INPROCEEDINGS{xu2018gaze, 
        author={Yanyu Xu and Yanbing Dong and Junru Wu and Zhengzhong Sun and Zhiru Shi and Jingyi Yu  and Shenghua Gao}, 
        booktitle={2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
        title={Gaze Prediction in Dynamic 360 Immersive Videos}, 
        year={2018} 
}</pre> -->
            </p>
        </div>

        <!-- disqus -->
        <br>
        <hr />
        <div id="disqus_thread"></div>
        <script>
            /**
             *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
             *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

            (function () { // DON'T EDIT BELOW THIS LINE
                var d = document,
                    s = d.createElement('script');
                s.src = 'https://shanghaitechcvdl.disqus.com/embed.js';
                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
            })();
        </script>
        <noscript>Please enable JavaScript to view the
            <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>

    </div>
    </div>

    <div class="footer"></div>
    <script src="/js/jquery.min.js"></script>
    <script src="/js/all.min.js"></script>

    <script>
        $(document).ready(function () {
            $(".footer").load("/common/footer.html");
            $(".navigation").load("/common/navigation.html");
        });
    </script>
</body>

</html>