<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="index, follow">

    <title>Shanghaitech Vision and Intelligent Perception(SVIP) LAB</title>
    <meta name="description" content="">
    <link rel="alternate" type="application/rss+xml" href="/feed.xml">

    <!-- Stylesheet -->
    <link rel="stylesheet" href="/css/main.css" rel='stylesheet' type='text/css'>


    <!-- Google Fonts -->
    <link href='https://fonts.googleapis.com/css?family=Nunito:400,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic' rel='stylesheet' type='text/css'>

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/small_logo.png">
</head>


<body data-spy="scroll" data-offset="80" data-target=".scrollspy" id="top">
    <div class="navigation"></div>


    <div class="news top-container">
        <div class="container-fluid">
            <div id="primarycontent">
                <center>
                    <h2>Personalized Saliency and its Prediction</h2>
                </center>
                <center>
                    <h3>
                        <a href="">Yanyu Xu</a>&nbsp;&nbsp;&nbsp;
                        <a href="">Junru Wu</a>&nbsp;&nbsp;&nbsp;
                        <a href="">Nianyi Li</a>&nbsp;&nbsp;&nbsp;
                        <a href="">Shenghua Gao</a>&nbsp;&nbsp;&nbsp;
                        <a href="">Jingyi Yu</a>
                    </h3>
                </center>
                <center>
                    <h3>
                        <a href="http://www.shanghaitech.edu.cn" target="_blank">ShanghaiTech University</a>
                    </h3>
                </center>
                <center>
                    <h3 style="color: #000000">Submitted to TPAMI (minor revision)</h2>
                </center>
                <!-- <center>
                    <h3>
                        <a href="https://arxiv.org/abs/1712.09867" target="_blank" style="color: #990000">[Paper]</a>
                        <a href="https://github.com/StevenLiuWen/ano_pred_cvpr2018" target="_blank" style="color: #990000">[Code (Tensorflow)]</a>
                    </h3>
                </center> -->
                <center>
                    <img src="../img/project/tpami2018_xuyy1.png" width="1000">
                </center>
                <p></p>


                <p>
                    <h2>Abstract</h2>

                    <div style="font-size:14px">
                        <p>Almost all existing visual saliency models focus on predicting a universal saliency map
                            across all
                            observers. Yet psychology studies suggest that visual attention of different observers can
                            vary
                            a lot under some specific circumstances, especially when they view scenes with multiple
                            salient
                            objects. However, few work explores this visual attention difference probably because of
                            lacking
                            a proper dataset, as well as complex correlation between visual attention, personal
                            preferences,
                            and image contents.
                        </p>
                    </div>

                    <h2>Introduction</h2>
                    <p>In this paper, we set out to study this heterogenous visual attention pattern between different
                        observers
                        and build the first dataset for personalized saliency detection. Further, we propose to
                        decompose
                        a personalized saliency map (referred to as PSM) into a universal saliency map (referred to as
                        USM)
                        which can be predicted by any existing saliency detection models and a discrepancy between
                        them.
                        Then personalized saliency detection is casted as the task of discrepancy estimation between
                        PSM
                        and USM. To tackle this task we propose two solutions: i) The discrepancy estimation for
                        different
                        observers are casted as different but related tasks. Then we feed the image and its USM into a
                        multi-task
                        convolutional neural network framework to estimate the discrepancy between PSM and USM for each
                        observer;
                        ii) As the discrepancy is related to both image contents and the observersâ€™ person-specific
                        information,
                        we feed the image and its associated USM into a convolutional neural network with
                        person-specific
                        information encoded filters to estimate the discrepancy. Extensive experimental results
                        demonstrate
                        the effectiveness of our models for PSM prediction as well their generalization capability for
                        unseen
                        observers.
                    </p>

                    <h2>FrameWork</h2>
                    <center>
                        <img src="../img/project/tpami2018_xuyy1.png" width="1000">
                    </center>
                    <center>
                        <img src="../img/project/tpami2018_xuyy1_2.png" width="1000">
                    </center>


                    <!-- <h2>Citation</h2>
                    If you find this useful, please cite our work as follows:

                    <pre>
@INPROCEEDINGS{xu2018gaze, 
        author={Yanyu Xu and Yanbing Dong and Junru Wu and Zhengzhong Sun and Zhiru Shi and Jingyi Yu  and Shenghua Gao}, 
        booktitle={2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
        title={Gaze Prediction in Dynamic 360 Immersive Videos}, 
        year={2018} 
}</pre> -->
                </p>
            </div>
            <!-- disqus -->
            <br>
            <hr />
            <div id="disqus_thread"></div>
            <script>
                /**
                 *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
                 *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

                (function () { // DON'T EDIT BELOW THIS LINE
                    var d = document,
                        s = d.createElement('script');
                    s.src = 'https://shanghaitechcvdl.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                })();
            </script>
            <noscript>Please enable JavaScript to view the
                <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
            </noscript>
        </div>
    </div>



    <div class="footer"></div>
    <script src="/js/jquery.min.js"></script>
    <script src="/js/all.min.js"></script>

    <script>
        $(document).ready(function () {
            $(".footer").load("/common/footer.html");
            $(".navigation").load("/common/navigation.html");
        });
    </script>
</body>

</html>